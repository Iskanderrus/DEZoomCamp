{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ef8a0c",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce0f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668bee2",
   "metadata": {},
   "source": [
    "# Initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cf6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/27 19:44:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/27 19:44:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('test') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f95205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-25 20:08:52--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 13.33.93.151, 13.33.93.32, 13.33.93.121, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|13.33.93.151|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 308924937 (295M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.parquet’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 294.61M  51.0MB/s    in 6.1s    \n",
      "\n",
      "2023-02-25 20:08:59 (48.6 MB/s) - ‘fhvhv_tripdata_2021-01.parquet’ saved [308924937/308924937]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a587fd",
   "metadata": {},
   "source": [
    "# Reading file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1216c233",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4284722558.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    .option('header', 'true') \\#this string allows spark accept the first line as header\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "        .option('header', 'true') \\#this string allows spark accept the first line as header\n",
    "        .option(\"InferSchema\", 'true') \\#this string allows to apply logical data types to the columns\n",
    "        .parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b37a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2021, 1, 1, 0, 28, 9), on_scene_datetime=datetime.datetime(2021, 1, 1, 0, 31, 42), pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, trip_miles=5.26, trip_time=923, base_passenger_fare=22.28, tolls=0.0, bcf=0.67, sales_tax=1.98, congestion_surcharge=2.75, airport_fee=None, tips=0.0, driver_pay=14.99, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea89a568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampType(), True), StructField('on_scene_datetime', TimestampType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb536d",
   "metadata": {},
   "source": [
    "# Partitioning the df and writing it to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c661066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca1203e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfhvhv/2021/01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91e542",
   "metadata": {},
   "source": [
    "# Reading parquet fies into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0142a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14676f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, originating_base_num: string, request_datetime: timestamp, on_scene_datetime: timestamp, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint, trip_miles: double, trip_time: bigint, base_passenger_fare: double, tolls: double, bcf: double, sales_tax: double, congestion_surcharge: double, airport_fee: double, tips: double, driver_pay: double, shared_request_flag: string, shared_match_flag: string, access_a_ride_flag: string, wav_request_flag: string, wav_match_flag: string]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce06b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03ed0141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting specific columns\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adfa84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-11 18:40:22|2021-01-11 19:15:49|         262|         231|\n",
      "|2021-01-05 15:13:22|2021-01-05 15:27:50|          61|         181|\n",
      "|2021-01-31 18:42:09|2021-01-31 18:59:52|         232|           4|\n",
      "|2021-01-27 22:24:36|2021-01-27 22:26:43|          68|          68|\n",
      "|2021-01-30 08:35:46|2021-01-30 08:39:42|         256|         255|\n",
      "|2021-01-16 02:25:35|2021-01-16 02:34:21|          89|          91|\n",
      "|2021-01-11 11:58:23|2021-01-11 12:14:19|          97|          61|\n",
      "|2021-01-03 07:44:58|2021-01-03 08:04:45|          26|         178|\n",
      "|2021-01-14 18:52:00|2021-01-14 19:19:00|         181|         198|\n",
      "|2021-01-08 20:35:35|2021-01-08 21:06:33|          76|          91|\n",
      "|2021-01-15 13:49:48|2021-01-15 14:35:23|         246|          16|\n",
      "|2021-01-27 10:37:56|2021-01-27 10:53:35|         135|          73|\n",
      "|2021-01-11 17:29:44|2021-01-11 17:42:49|          68|         211|\n",
      "|2021-01-24 21:32:15|2021-01-24 21:52:42|         249|         236|\n",
      "|2021-01-26 18:03:39|2021-01-26 18:10:53|          79|           4|\n",
      "|2021-01-07 08:05:32|2021-01-07 08:30:47|          22|          25|\n",
      "|2021-01-12 17:16:00|2021-01-12 17:25:56|          69|         119|\n",
      "|2021-01-16 21:00:39|2021-01-16 21:18:15|         239|         239|\n",
      "|2021-01-14 23:35:14|2021-01-14 23:45:30|          61|          62|\n",
      "|2021-01-17 23:23:11|2021-01-17 23:34:42|         241|          20|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    ".filter(df.hvfhs_license_num == 'HV0003') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc824cd0",
   "metadata": {},
   "source": [
    "# Functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d108e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing predefined functions in spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58e2ed6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-11|  2021-01-11|         262|         231|\n",
      "| 2021-01-05|  2021-01-05|          61|         181|\n",
      "| 2021-01-02|  2021-01-02|         100|           1|\n",
      "| 2021-01-31|  2021-01-31|         232|           4|\n",
      "| 2021-01-05|  2021-01-05|         162|           1|\n",
      "| 2021-01-27|  2021-01-27|          68|          68|\n",
      "| 2021-01-18|  2021-01-18|         205|         205|\n",
      "| 2021-01-30|  2021-01-30|         256|         255|\n",
      "| 2021-01-16|  2021-01-16|          89|          91|\n",
      "| 2021-01-05|  2021-01-05|         132|         102|\n",
      "| 2021-01-11|  2021-01-11|          97|          61|\n",
      "| 2021-01-22|  2021-01-22|          79|          37|\n",
      "| 2021-01-03|  2021-01-03|          26|         178|\n",
      "| 2021-01-14|  2021-01-14|         181|         198|\n",
      "| 2021-01-08|  2021-01-08|          76|          91|\n",
      "| 2021-01-15|  2021-01-15|         246|          16|\n",
      "| 2021-01-27|  2021-01-27|         135|          73|\n",
      "| 2021-01-18|  2021-01-18|          74|         234|\n",
      "| 2021-01-11|  2021-01-11|          68|         211|\n",
      "| 2021-01-24|  2021-01-24|         249|         236|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    ".select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a7aee",
   "metadata": {},
   "source": [
    "# User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa2ce453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_func(base_num): \n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0: \n",
    "        return f's/{num:03x}'\n",
    "    else: \n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18c61f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crasy_func_udf = F.udf(crazy_func, returnType=T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7b2f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/acc| 2021-01-11|  2021-01-11|         262|         231|\n",
      "|  e/a39| 2021-01-05|  2021-01-05|          61|         181|\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|         100|           1|\n",
      "|  e/b42| 2021-01-31|  2021-01-31|         232|           4|\n",
      "|  s/af0| 2021-01-05|  2021-01-05|         162|           1|\n",
      "|  e/b43| 2021-01-27|  2021-01-27|          68|          68|\n",
      "|  e/9ce| 2021-01-18|  2021-01-18|         205|         205|\n",
      "|  e/b35| 2021-01-30|  2021-01-30|         256|         255|\n",
      "|  e/b3b| 2021-01-16|  2021-01-16|          89|          91|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         132|         102|\n",
      "|  e/acc| 2021-01-11|  2021-01-11|          97|          61|\n",
      "|  e/9ce| 2021-01-22|  2021-01-22|          79|          37|\n",
      "|  e/b32| 2021-01-03|  2021-01-03|          26|         178|\n",
      "|  e/b49| 2021-01-14|  2021-01-14|         181|         198|\n",
      "|  e/acc| 2021-01-08|  2021-01-08|          76|          91|\n",
      "|  e/b3c| 2021-01-15|  2021-01-15|         246|          16|\n",
      "|  s/b36| 2021-01-27|  2021-01-27|         135|          73|\n",
      "|  e/9ce| 2021-01-18|  2021-01-18|          74|         234|\n",
      "|  e/a39| 2021-01-11|  2021-01-11|          68|         211|\n",
      "|  e/b3f| 2021-01-24|  2021-01-24|         249|         236|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    ".withColumn('base_id', crasy_func_udf(df.dispatching_base_num)) \\\n",
    ".select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7eb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2e85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e37882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796f6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283e325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
